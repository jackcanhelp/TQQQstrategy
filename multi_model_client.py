"""
Multi-Model AI Decision Engine
==============================
ä½¿ç”¨ GitHub Models API å¯¦ç¾å¤šæ¨¡å‹æ±ºç­–å¼•æ“ã€‚
æ”¯æ´ GPT-4.1 â†’ DeepSeek-V3 â†’ Llama-4-Scout å±¤ç´šå¼ failoverã€‚
"""

import os
import json
import time
import logging
from typing import Optional, Dict, Any
from dataclasses import dataclass
from dotenv import load_dotenv

load_dotenv()

# è¨­ç½® logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@dataclass
class TradingDecision:
    """æ¨™æº–åŒ–çš„äº¤æ˜“æ±ºç­–è¼¸å‡ºã€‚"""
    signal: str  # "BUY", "SELL", "HOLD"
    confidence_score: float  # 0.0 - 1.0
    reasoning_summary: str
    model_used: str

    def to_dict(self) -> Dict:
        return {
            "signal": self.signal,
            "confidence_score": self.confidence_score,
            "reasoning_summary": self.reasoning_summary,
            "model_used": self.model_used
        }

    def to_json(self) -> str:
        return json.dumps(self.to_dict(), indent=2)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ¨¡å‹å±¤ç´šé…ç½®
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MODEL_HIERARCHY = [
    {
        "name": "gpt-4.1",
        "model_id": "openai/gpt-4.1",  # GitHub Models ä¸Šæœ€å¼· OpenAI æ¨¡å‹
        "role": "Lead Strategist",
        "description": "Logic-heavy and multi-step tasks",
        "timeout": 60,
        "max_tokens": 1000,
    },
    {
        "name": "DeepSeek-V3",
        "model_id": "deepseek/DeepSeek-V3-0324",
        "role": "Check & Balance / Failover",
        "description": "High-performance alternative for trend validation",
        "timeout": 45,
        "max_tokens": 1000,
    },
    {
        "name": "Llama-4-Scout-17B",
        "model_id": "meta/Llama-4-Scout-17B-16E-Instruct",
        "role": "Safety Net",
        "description": "Multi-document processing fallback",
        "timeout": 30,
        "max_tokens": 800,
    },
]


class MultiModelClient:
    """
    å¤šæ¨¡å‹ AI æ±ºç­–å¼•æ“ã€‚

    ä½¿ç”¨ GitHub Models APIï¼Œä¾åºå˜—è©¦ï¼š
    1. GPT-4.1 (ä¸»è¦)
    2. DeepSeek-V3 (å‚™ç”¨)
    3. Llama-4-Scout (æœ€çµ‚å‚™ç”¨)
    """

    GITHUB_MODELS_ENDPOINT = "https://models.github.ai/inference"

    def __init__(self):
        """åˆå§‹åŒ– Multi-Model Clientã€‚"""
        self.token = os.getenv("GITHUB_TOKEN")
        if not self.token:
            raise ValueError("GITHUB_TOKEN not found in environment variables")

        self.client = None
        self._init_client()

        # çµ±è¨ˆ
        self.stats = {
            "total_requests": 0,
            "model_usage": {m["name"]: 0 for m in MODEL_HIERARCHY},
            "failures": {m["name"]: 0 for m in MODEL_HIERARCHY},
        }

        logger.info("ğŸ¤– Multi-Model Client åˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   æ¨¡å‹å±¤ç´š: {' â†’ '.join(m['name'] for m in MODEL_HIERARCHY)}")

    def _init_client(self):
        """åˆå§‹åŒ– OpenAI ç›¸å®¹å®¢æˆ¶ç«¯ã€‚"""
        try:
            from openai import OpenAI
            self.client = OpenAI(
                base_url=self.GITHUB_MODELS_ENDPOINT,
                api_key=self.token,
            )
            logger.info("âœ… OpenAI SDK å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸ")
        except ImportError:
            logger.warning("âš ï¸ OpenAI SDK æœªå®‰è£ï¼Œå˜—è©¦ä½¿ç”¨ Azure AI Inference SDK")
            try:
                from azure.ai.inference import ChatCompletionsClient
                from azure.core.credentials import AzureKeyCredential
                self.client = ChatCompletionsClient(
                    endpoint=self.GITHUB_MODELS_ENDPOINT,
                    credential=AzureKeyCredential(self.token),
                )
                logger.info("âœ… Azure AI Inference SDK å®¢æˆ¶ç«¯åˆå§‹åŒ–æˆåŠŸ")
            except ImportError:
                raise ImportError(
                    "è«‹å®‰è£ openai æˆ– azure-ai-inference: "
                    "pip install openai æˆ– pip install azure-ai-inference"
                )

    def _build_trading_prompt(self, market_data: Dict) -> str:
        """å»ºæ§‹äº¤æ˜“æ±ºç­–çš„ promptã€‚"""
        return f"""You are an expert quantitative trading advisor for TQQQ (3x Leveraged Nasdaq ETF).

MARKET DATA:
{json.dumps(market_data, indent=2)}

ANALYSIS REQUIREMENTS:
1. Evaluate the current market regime (Bull/Bear/Sideways)
2. Assess volatility levels and VIX indicators
3. Consider trend strength and momentum
4. Factor in the 3x leverage decay risk of TQQQ

CRITICAL RULES FOR TQQQ:
- In high volatility (VIX > 25): Prefer HOLD or SELL
- In strong downtrends: SELL to avoid 3x losses
- Only BUY in confirmed uptrends with low volatility
- SURVIVAL is more important than PROFIT

OUTPUT FORMAT (STRICTLY JSON, no other text):
{{
    "signal": "BUY" or "SELL" or "HOLD",
    "confidence_score": 0.0 to 1.0,
    "reasoning_summary": "Concise 1-2 sentence explanation"
}}

Respond ONLY with the JSON object, no additional text."""

    def _parse_response(self, response_text: str, model_name: str) -> Optional[TradingDecision]:
        """è§£ææ¨¡å‹å›æ‡‰ç‚ºæ¨™æº–åŒ–æ±ºç­–ã€‚"""
        try:
            # å˜—è©¦ç›´æ¥è§£æ JSON
            # ç§»é™¤å¯èƒ½çš„ markdown åŒ…è£
            text = response_text.strip()
            if text.startswith("```json"):
                text = text[7:]
            if text.startswith("```"):
                text = text[3:]
            if text.endswith("```"):
                text = text[:-3]
            text = text.strip()

            data = json.loads(text)

            # é©—è­‰å¿…è¦æ¬„ä½
            signal = data.get("signal", "HOLD").upper()
            if signal not in ["BUY", "SELL", "HOLD"]:
                signal = "HOLD"

            confidence = float(data.get("confidence_score", 0.5))
            confidence = max(0.0, min(1.0, confidence))

            reasoning = data.get("reasoning_summary", "No reasoning provided")

            return TradingDecision(
                signal=signal,
                confidence_score=confidence,
                reasoning_summary=reasoning,
                model_used=model_name
            )

        except (json.JSONDecodeError, KeyError, ValueError) as e:
            logger.warning(f"âš ï¸ ç„¡æ³•è§£æ {model_name} å›æ‡‰: {e}")
            return None

    def _call_model(self, model_config: Dict, prompt: str) -> Optional[str]:
        """å‘¼å«å–®ä¸€æ¨¡å‹ã€‚"""
        model_name = model_config["name"]
        model_id = model_config["model_id"]
        timeout = model_config["timeout"]
        max_tokens = model_config["max_tokens"]

        try:
            logger.info(f"ğŸ”„ å˜—è©¦å‘¼å« {model_name} ({model_config['role']})...")

            # ä½¿ç”¨ OpenAI SDK æ ¼å¼
            if hasattr(self.client, 'chat'):
                response = self.client.chat.completions.create(
                    model=model_id,
                    messages=[
                        {"role": "system", "content": "You are a quantitative trading expert. Always respond in valid JSON format."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=max_tokens,
                    temperature=0.3,  # è¼ƒä½æº«åº¦ä»¥ç²å¾—ä¸€è‡´æ€§
                    timeout=timeout,
                )
                return response.choices[0].message.content
            else:
                # Azure SDK æ ¼å¼
                from azure.ai.inference.models import SystemMessage, UserMessage
                response = self.client.complete(
                    model=model_id,
                    messages=[
                        SystemMessage(content="You are a quantitative trading expert. Always respond in valid JSON format."),
                        UserMessage(content=prompt)
                    ],
                    max_tokens=max_tokens,
                    temperature=0.3,
                )
                return response.choices[0].message.content

        except Exception as e:
            error_type = type(e).__name__
            logger.warning(f"âŒ {model_name} å¤±æ•— ({error_type}): {str(e)[:100]}")
            self.stats["failures"][model_name] += 1
            # Rate limit æ™‚ç­‰å¾…ä¸€ä¸‹å†å˜—è©¦ä¸‹ä¸€å€‹æ¨¡å‹
            if 'RateLimit' in error_type or '429' in str(e):
                time.sleep(5)
            return None

    def get_trading_decision(self, market_data: Dict) -> TradingDecision:
        """
        å–å¾—äº¤æ˜“æ±ºç­–ï¼Œä¾åºå˜—è©¦å„æ¨¡å‹ã€‚

        Args:
            market_data: å¸‚å ´æ•¸æ“šå­—å…¸ï¼ŒåŒ…å«åƒ¹æ ¼ã€æŒ‡æ¨™ç­‰

        Returns:
            TradingDecision ç‰©ä»¶
        """
        self.stats["total_requests"] += 1
        prompt = self._build_trading_prompt(market_data)

        # ä¾åºå˜—è©¦å„æ¨¡å‹
        for model_config in MODEL_HIERARCHY:
            model_name = model_config["name"]

            response_text = self._call_model(model_config, prompt)

            if response_text:
                decision = self._parse_response(response_text, model_name)
                if decision:
                    self.stats["model_usage"][model_name] += 1
                    logger.info(f"âœ… {model_name} æˆåŠŸ: {decision.signal} (ä¿¡å¿ƒåº¦: {decision.confidence_score:.2f})")
                    return decision

        # æ‰€æœ‰æ¨¡å‹éƒ½å¤±æ•—ï¼Œè¿”å›å®‰å…¨é è¨­å€¼
        logger.error("âŒ æ‰€æœ‰æ¨¡å‹éƒ½å¤±æ•—ï¼Œè¿”å›å®‰å…¨é è¨­å€¼ (HOLD)")
        return TradingDecision(
            signal="HOLD",
            confidence_score=0.0,
            reasoning_summary="All models failed, defaulting to safe HOLD position",
            model_used="fallback"
        )

    def get_strategy_idea(self, context: str, indicators: str) -> Optional[str]:
        """
        ä½¿ç”¨å¤šæ¨¡å‹ç”Ÿæˆç­–ç•¥æƒ³æ³•ï¼ˆæ•´åˆåˆ°ç¾æœ‰æ¼”åŒ–ç³»çµ±ï¼‰ã€‚

        Args:
            context: æ­·å²ç­–ç•¥ä¸Šä¸‹æ–‡
            indicators: å¿…é ˆä½¿ç”¨çš„æŒ‡æ¨™

        Returns:
            ç­–ç•¥æƒ³æ³•æ–‡å­—
        """
        prompt = f"""You are a Quantitative Research Director designing TQQQ trading strategies.

CONTEXT:
{context}

{indicators}

Generate a NEW trading strategy idea. Focus on:
1. Regime Filter (when to stay in cash)
2. Entry Signal (when to buy)
3. Exit Rules (when to sell)

Use ONLY backward-looking indicators. NO look-ahead bias.
Keep the response concise and actionable."""

        for model_config in MODEL_HIERARCHY:
            response = self._call_model(model_config, prompt)
            if response:
                logger.info(f"âœ… ç­–ç•¥æƒ³æ³•ç”± {model_config['name']} ç”Ÿæˆ")
                return response

        return None

    def generate(self, prompt: str) -> Optional[str]:
        """
        Public interface: ä¾åºå˜—è©¦å„æ¨¡å‹ï¼Œè¿”å›ç¬¬ä¸€å€‹æˆåŠŸçš„åŸå§‹å›æ‡‰æ–‡å­—ã€‚
        ä¾› researcher.py ç­‰å¤–éƒ¨æ¨¡çµ„ä½¿ç”¨ã€‚
        """
        for model_config in MODEL_HIERARCHY:
            response = self._call_model(model_config, prompt)
            if response:
                logger.info(f"âœ… GitHub Models ({model_config['name']}) å›æ‡‰æˆåŠŸ")
                return response
        return None

    def _call_model_chain(self, prompt: str) -> Optional[str]:
        """Deprecated alias for generate(). Use generate() instead."""
        return self.generate(prompt)

    def get_stats(self) -> str:
        """å–å¾—ä½¿ç”¨çµ±è¨ˆã€‚"""
        lines = [
            "ğŸ¤– Multi-Model Client çµ±è¨ˆ:",
            f"   ç¸½è«‹æ±‚æ•¸: {self.stats['total_requests']}",
            "",
            "   æ¨¡å‹ä½¿ç”¨:",
        ]
        for name, count in self.stats["model_usage"].items():
            failures = self.stats["failures"][name]
            lines.append(f"      {name}: {count} æˆåŠŸ, {failures} å¤±æ•—")

        return "\n".join(lines)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ä¾¿æ·å‡½æ•¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_client = None

def get_multi_model_client() -> MultiModelClient:
    """å–å¾—å…¨åŸŸ MultiModelClient å¯¦ä¾‹ã€‚"""
    global _client
    if _client is None:
        _client = MultiModelClient()
    return _client


def get_trading_decision(market_data: Dict) -> TradingDecision:
    """
    ä¾¿æ·å‡½æ•¸ï¼šå–å¾—äº¤æ˜“æ±ºç­–ã€‚

    Args:
        market_data: å¸‚å ´æ•¸æ“š

    Returns:
        TradingDecision
    """
    client = get_multi_model_client()
    return client.get_trading_decision(market_data)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ¸¬è©¦
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    # æ¸¬è©¦ç”¨å¸‚å ´æ•¸æ“š
    test_market_data = {
        "date": "2024-01-15",
        "tqqq_price": 45.23,
        "tqqq_change_pct": -2.1,
        "qqq_price": 410.50,
        "vix": 18.5,
        "sma_50": 44.00,
        "sma_200": 42.50,
        "rsi_14": 45.2,
        "atr_14": 1.85,
        "trend": "neutral",
        "volume_ratio": 1.2
    }

    print("=" * 60)
    print("ğŸ§ª æ¸¬è©¦ Multi-Model Client")
    print("=" * 60)

    try:
        decision = get_trading_decision(test_market_data)
        print("\nğŸ“Š äº¤æ˜“æ±ºç­–:")
        print(decision.to_json())

        client = get_multi_model_client()
        print("\n" + client.get_stats())

    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {e}")
